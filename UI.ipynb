{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gradio\n",
      "Version: 4.19.2\n",
      "Summary: Python library for easily interacting with trained machine learning models\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Abubakar Abid <gradio-team@huggingface.co>, Ali Abid <gradio-team@huggingface.co>, Ali Abdalla <gradio-team@huggingface.co>, Dawood Khan <gradio-team@huggingface.co>, Ahsen Khaliq <gradio-team@huggingface.co>, Pete Allen <gradio-team@huggingface.co>, √ñmer Faruk √ñzdemir <gradio-team@huggingface.co>, Freddy A Boulton <gradio-team@huggingface.co>, Hannah Blair <gradio-team@huggingface.co>\n",
      "License: \n",
      "Location: d:\\001drexel\\12_dsci_capstone\\dsci_capstone\\.conda\\lib\\site-packages\n",
      "Requires: aiofiles, altair, fastapi, ffmpy, gradio-client, httpx, huggingface-hub, importlib-resources, jinja2, markupsafe, matplotlib, numpy, orjson, packaging, pandas, pillow, pydantic, pydub, python-multipart, pyyaml, ruff, semantic-version, tomlkit, typer, typing-extensions, uvicorn\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import re\n",
    "\n",
    "from util import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import importlib\n",
    "import logging\n",
    "import yaml\n",
    "from main import *\n",
    "\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 222.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded np array\n",
      " with X shape: (775, 17, 216)\n",
      " with y shape: (775,)\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned model loaded\n",
      "Input signal loaded\n",
      "Preprocess finished:  (1, 17, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 116.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_gradio_audio(audio_input,target_sr):\n",
    "    origin_sr,origin_signal = audio_input\n",
    "    if len(origin_signal.shape) == 2:\n",
    "        origin_signal = origin_signal.T[0]\n",
    "    origin_signal = origin_signal.astype(np.float32)\n",
    "    signal = librosa.resample(origin_signal, orig_sr=origin_sr, target_sr=target_sr)\n",
    "    return signal\n",
    "\n",
    "def main():\n",
    "    # ===Setting for all parameters===\n",
    "    sample_rate = 22050\n",
    "    segment_length = 5\n",
    "    n_fft = 2048\n",
    "    n_mels = 128\n",
    "    n_mfcc = 17\n",
    "    params_path = \"params.yml\"\n",
    "    with open(params_path, \"r\") as file:\n",
    "        params = yaml.safe_load(file)\n",
    "\n",
    "    algo = params[\"algorithm\"]\n",
    "    batch_size = int(algo[\"params\"][\"batch_size\"])\n",
    "    n_epochs = 50\n",
    "    patience = int(algo[\"params\"][\"patience\"])\n",
    "    learning_rate = float(algo[\"params\"][\"learning_rate\"])\n",
    "\n",
    "    model_dir = \"./model/auth/app\"\n",
    "    model_save_path = os.path.join(model_dir, \"demo.pkl\")\n",
    "    demo_model_path = \"./model/auth/demo\"\n",
    "\n",
    "    device = get_cuda_device()\n",
    "    \n",
    "    # CNN\n",
    "    PATH = \"./model/basecnn200\"\n",
    "    model_set = torch.load(f\"{PATH}/model.pt\")\n",
    "    clf_pre = model_set[\"model\"]\n",
    "\n",
    "    DATA_ADDRESS = \"./data\"\n",
    "    NON_SPEAKER_DIR = os.path.join(DATA_ADDRESS, \"preprocessed\", \"test\")\n",
    "    X_false, _ = load_data(\n",
    "        dir_feature=NON_SPEAKER_DIR,\n",
    "        file_prefix=\"source_mfcc_len5_fft2048_mels128_mfcc17_\",\n",
    "        dir_df_index=os.path.join(DATA_ADDRESS, \"df_index_source_test.pkl\"),\n",
    "        n_interval=500,\n",
    "        flatten=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # ===UI functions===\n",
    "    def train_model_pipeline(audio_input,denoise,model_name):\n",
    "        output = \"Train pipline starting\"\n",
    "        yield output\n",
    "        # Use the gr.Progress() to show the progress of training\n",
    "        speaker_auth_model = auth_model(clf=clf_pre, X_false=X_false, batch_size=batch_size)\n",
    "        output += \"\\nBase model loaded\"\n",
    "        yield output\n",
    "        signal = process_gradio_audio(audio_input,sample_rate)\n",
    "        output += \"\\nInput signal loaded\"\n",
    "        yield output\n",
    "\n",
    "        speaker_audio_loader = audio_loader(ls_raw_signal=[signal], sr=sample_rate)\n",
    "        speaker_audio_loader.process_raw(\n",
    "            segment_length=segment_length,\n",
    "            denoise=denoise,\n",
    "            n_fft=n_fft,\n",
    "            n_mels=n_mels,\n",
    "            n_mfcc=n_mfcc,\n",
    "        )\n",
    "        output += \"\\nFinished Preprocessing\\nModel Training...\"\n",
    "        yield output\n",
    "        \n",
    "        speaker_auth_model.train(\n",
    "            train_audio_loader=speaker_audio_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            patience=patience,\n",
    "            model_save_path=model_save_path,\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        output += \"\\nFinished training\"\n",
    "        yield output\n",
    "\n",
    "        model_dir = os.path.join(demo_model_path,model_name)\n",
    "        save_pickle(model_dir,speaker_auth_model)\n",
    "        print(\"model saved to:\",model_dir)\n",
    "        output += f\"\\nmodel saved to:{model_dir}\"\n",
    "        yield output\n",
    "        output +=\"\\nTraining completed successfully!\"\n",
    "        yield output\n",
    "\n",
    "    def fit_model_pipeline(audio_input, denoise, model_name,accept_threshold):\n",
    "        # signal,_ = read_signal(audio_input,sample_rate)\n",
    "        model_dir = os.path.join(demo_model_path,model_name)\n",
    "        speaker_auth_model = load_pickle(model_dir)\n",
    "        print(\"Fine tuned model loaded\")\n",
    "        signal = process_gradio_audio(audio_input,sample_rate)\n",
    "        print(\"Input signal loaded\")\n",
    "\n",
    "        speaker_audio_loader = audio_loader(ls_raw_signal=[signal], sr=sample_rate)\n",
    "        speaker_audio_loader.process_raw(\n",
    "            segment_length=segment_length,\n",
    "            denoise=denoise,\n",
    "            n_fft=n_fft,\n",
    "            n_mels=n_mels,\n",
    "            n_mfcc=n_mfcc,\n",
    "        )\n",
    "        \n",
    "        pred_list, prob_list = speaker_auth_model.predict(speaker_audio_loader = speaker_audio_loader, batch_size=batch_size, accept_threshold=accept_threshold)\n",
    "        avg_prob = np.average(prob_list[0])\n",
    "        if avg_prob >= accept_threshold:\n",
    "            pred = \"Authorized!üòÅ\"\n",
    "        else:\n",
    "            pred = \"Denied!üò†\"\n",
    "        prob = \"{:.4%}\".format(avg_prob)\n",
    "        return pred, prob\n",
    "\n",
    "    \n",
    "    # ===UI===\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Tab(\"Enrollment\"):\n",
    "            gr.Markdown(\"Please record or upload your enrollment audio down below, then click the `train` button.\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    audio_input = gr.Audio(label=\"Audio Input for authentication\")\n",
    "                    denoise = gr.Checkbox(label=\"Denoise\",value=True)                \n",
    "                with gr.Column():\n",
    "                    train_button = gr.Button(\"Train\")\n",
    "                    train_output = gr.Textbox(label=\"Training Status\")\n",
    "            gr.Markdown(\"Fancy UI for nerd ‚Üì\")\n",
    "            with gr.Row():\n",
    "                with gr.Group():\n",
    "                    # audio_input_dir = gr.Textbox(label=\"Audio File Directory (If you have multiple files)\")\n",
    "                    model_name = gr.Textbox(label=\"Model Name\",value=\"demo.pkl\")\n",
    "                    # gr.Markdown(\"blablabla blablabla\")\n",
    "\n",
    "            train_button.click(train_model_pipeline, inputs=[audio_input, denoise, model_name], outputs=train_output)\n",
    "\n",
    "        with gr.Tab(\"Authentication\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    test_audio_input = gr.Audio(label=\"Audio Input for authentication\")\n",
    "                    # test_audio_input = gr.Textbox(label=\"Audio File Directory\")\n",
    "                    test_denoise = gr.Checkbox(label=\"Denoise\",value=True)\n",
    "                    threshold = gr.Number(label=\"Accpetance threshold\",value=0.85)\n",
    "                    evaluate_button = gr.Button(\"Auth\")\n",
    "                with gr.Column():\n",
    "                    fit_result = gr.Textbox(label=\"Verification successful?\",value=\"?\")\n",
    "                    confidence = gr.Textbox(label=\"Voice verification pass rate\",value= 0)\n",
    "            gr.Markdown(\"---\")    \n",
    "            gr.Markdown(\"Other stuff may not needed\")    \n",
    "            with gr.Row():\n",
    "                test_model_dir = gr.Textbox(label=\"Saved Model Directory\",value=\"demo.pkl\")\n",
    "\n",
    "            evaluate_button.click(fit_model_pipeline, inputs=[test_audio_input, test_denoise, test_model_dir,threshold], outputs=[fit_result, confidence])\n",
    "\n",
    "        # with gr.Tab(\"Status\"):\n",
    "        #     gr.Markdown(\"Training status\")\n",
    "        #     with gr.Row():\n",
    "        #         train_accuracy = gr.Textbox(label=\"Training Accuracy\",value=np.nan)\n",
    "        #         train_eer = gr.Textbox(label=\"Training EER\",value=np.nan)\n",
    "        #         train_data_length = gr.Textbox(label=\"Length of Training Data\")\n",
    "        #         test_data_length = gr.Textbox(label=\"Length of Test Data\")\n",
    "        #     gr.Markdown(\"Training status\")\n",
    "        #     with gr.Row():\n",
    "        #         gr.Text(\"Train\")\n",
    "        #     with gr.Column():\n",
    "        #         refresh_button = gr.Button(\"Refresh\")\n",
    "        #     refresh_button.click(foo,outputs=[train_accuracy,train_eer])\n",
    "\n",
    "    demo.launch()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_pickle(\"./model/auth/demo/demo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned model loaded\n",
      "Input signal loaded\n",
      "Preprocess finished:  (1, 17, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0], dtype=int64)]\n",
      "[array([6.1316427e-06], dtype=float32)]\n",
      "Fine tuned model loaded\n",
      "Input signal loaded\n",
      "Preprocess finished:  (3, 17, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 99.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1, 1], dtype=int64)]\n",
      "[array([0.9987658 , 0.99749905, 0.9161894 ], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
